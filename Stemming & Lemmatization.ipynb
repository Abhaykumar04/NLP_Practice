{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78394c14",
   "metadata": {},
   "source": [
    "# Stemming & Lemmatization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a94e9",
   "metadata": {},
   "source": [
    "## Example: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0ef72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['stem', 'reduc', 'word', 'to', 'their', 'base', 'form', ',', 'while', 'lemmat', 'doe', 'so', 'with', 'the', 'help', 'of', 'a', 'dictionari', '.']\n",
      "Lemmatized Words: ['Stemming', 'reduces', 'word', 'to', 'their', 'base', 'form', ',', 'while', 'lemmatization', 'doe', 'so', 'with', 'the', 'help', 'of', 'a', 'dictionary', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Stemming reduces words to their base form, while lemmatization does so with the help of a dictionary.\"\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3426cc",
   "metadata": {},
   "source": [
    "## Example: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded0d31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['snowballstemm', 'support', 'multipl', 'languag', 'for', 'stem', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"SnowballStemmer supports multiple languages for stemming.\"\n",
    "\n",
    "# Stemming with SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a6013c",
   "metadata": {},
   "source": [
    "## Example: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c62478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['lancasterstem', 'is', 'an', 'aggress', 'stem', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"LancasterStemmer is an aggressive stemming algorithm.\"\n",
    "\n",
    "# Stemming with LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9f670",
   "metadata": {},
   "source": [
    "## Example: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83020aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['stem', 'and', 'lemmat', 'may', 'not', 'alway', 'produc', 'the', 'same', 'result', '.']\n",
      "Lemmatized Words: ['Stemming', 'and', 'lemmatization', 'may', 'not', 'always', 'produce', 'the', 'same', 'result', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Stemming and lemmatization may not always produce the same results.\"\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731a174",
   "metadata": {},
   "source": [
    "## Example: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b88bedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['WordNetLemmatizer', 'us', 'a', 'lexical', 'database', 'for', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"WordNetLemmatizer uses a lexical database for lemmatization.\"\n",
    "\n",
    "# Lemmatization with WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac1658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
